{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现一层神经元数值的向前传播和梯度的反向传播算法\n",
    "\n",
    "1. 输入参数：输入向量x，权重矩阵W，偏置项b 向量，激活函数f\n",
    "2. 计算输出值：输出值y=f(Wx+b)\n",
    "4. 计算梯度：\n",
    "   下一层传回误差: R\n",
    "   梯度dy/dW= x * (f'(Wx+b * R)) \n",
    "   dy/db= f'(Wx+b)* R\n",
    "   向前传播误差: dy/dx= f'(Wx+b)* R * W \n",
    "\n",
    "   \n",
    "\n",
    "5. 激活函数f 取sigmoid函数 f(x)=1/(1+exp(-x))\n",
    "   sigmoid函数的导数:f'(x)=sigmoid(x)(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size,  activation=sigmoid, derivative=sigmoid_derivative):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros(output_size)\n",
    "        self.output = None\n",
    "        self.dinputs = None\n",
    "        self.dweights = None\n",
    "        self.dbiases = None\n",
    "        self.activation = activation\n",
    "        self.derivative = derivative\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        samples = len(dvalues)\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        return self.dinputs\n",
    "    \n",
    "    def update(self,learning_rate,samples):\n",
    "        self.weights -= learning_rate * self.dweights / samples\n",
    "        self.biases -= learning_rate * self.dbiases / samples\n",
    "\n",
    "\n",
    "class LinearNetwork:\n",
    "    def __init__(self,  activation=sigmoid, derivative=sigmoid_derivative):\n",
    "        self.activation = activation\n",
    "        self.derivative = derivative\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.01\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        curr_input = inputs\n",
    "        for layer in self.layers:\n",
    "            layer.forward(curr_input)\n",
    "            curr_input = layer.activation(layer.output)\n",
    "        return curr_input\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        n_samples, n_features = dvalues.shape\n",
    "        curr_dvalues = dvalues\n",
    "        for layer in reversed(self.layers):\n",
    "            curr_dvalues = layer.derivative(layer.output) * curr_dvalues\n",
    "            curr_dvalues = layer.backward(curr_dvalues)\n",
    "\n",
    "    def update(self,n_samples):\n",
    "        for layer in self.layers:\n",
    "            layer.update(self.learning_rate, n_samples)\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        n_samples, n_features = inputs.shape\n",
    "        output = self.forward(inputs)\n",
    "        self.backward(output - targets)\n",
    "        self.update(n_samples)\n",
    "        loss = np.mean(np.square(output - targets))        \n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, batch_size=10):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                batch_X = X[i:i+batch_size]\n",
    "                batch_y = y[i:i+batch_size]\n",
    "                loss = self.train(batch_X, batch_y)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} loss: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 4), (45, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_test_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.5326681501727518\n",
      "Epoch 100 loss: 0.2597462011984709\n",
      "Epoch 200 loss: 0.19623773926570465\n",
      "Epoch 300 loss: 0.17172748077485822\n",
      "Epoch 400 loss: 0.1567052070800773\n",
      "Epoch 500 loss: 0.14686236993718418\n",
      "Epoch 600 loss: 0.13978974571300304\n",
      "Epoch 700 loss: 0.13408096034022404\n",
      "Epoch 800 loss: 0.1291628831700563\n",
      "Epoch 900 loss: 0.12476031698621508\n",
      "Epoch 1000 loss: 0.1207187946150598\n",
      "Epoch 1100 loss: 0.11694243754687497\n",
      "Epoch 1200 loss: 0.11336612981460917\n",
      "Epoch 1300 loss: 0.10994272601053331\n",
      "Epoch 1400 loss: 0.10663688063896151\n",
      "Epoch 1500 loss: 0.10342154238931549\n",
      "Epoch 1600 loss: 0.10027538198740617\n",
      "Epoch 1700 loss: 0.09718036032635691\n",
      "Epoch 1800 loss: 0.09411876678348631\n",
      "Epoch 1900 loss: 0.09106843861099018\n",
      "Epoch 2000 loss: 0.08799275975807024\n",
      "Epoch 2100 loss: 0.08481558556811138\n",
      "Epoch 2200 loss: 0.08135748358486322\n",
      "Epoch 2300 loss: 0.07730016575700166\n",
      "Epoch 2400 loss: 0.07298628152101345\n",
      "Epoch 2500 loss: 0.0694864948567319\n",
      "Epoch 2600 loss: 0.06666559526971871\n",
      "Epoch 2700 loss: 0.06419696069648492\n",
      "Epoch 2800 loss: 0.06192050488370334\n",
      "Epoch 2900 loss: 0.05976157437498344\n",
      "Epoch 3000 loss: 0.05768593745846178\n",
      "Epoch 3100 loss: 0.055678587570644617\n",
      "Epoch 3200 loss: 0.05373355058713866\n",
      "Epoch 3300 loss: 0.051848955830019194\n",
      "Epoch 3400 loss: 0.05002464742814157\n",
      "Epoch 3500 loss: 0.04826102666276671\n",
      "Epoch 3600 loss: 0.046558497646405156\n",
      "Epoch 3700 loss: 0.0449172161194003\n",
      "Epoch 3800 loss: 0.0433369955829168\n",
      "Epoch 3900 loss: 0.041817296455008375\n",
      "Epoch 4000 loss: 0.04035725731433303\n",
      "Epoch 4100 loss: 0.03895574420975457\n",
      "Epoch 4200 loss: 0.03761140396571095\n",
      "Epoch 4300 loss: 0.03632271397420498\n",
      "Epoch 4400 loss: 0.03508802529418788\n",
      "Epoch 4500 loss: 0.03390559846970722\n",
      "Epoch 4600 loss: 0.03277363276768457\n",
      "Epoch 4700 loss: 0.03169028997786174\n",
      "Epoch 4800 loss: 0.030653713900182854\n",
      "Epoch 4900 loss: 0.02966204644000708\n",
      "Epoch 5000 loss: 0.028713440994078136\n",
      "Epoch 5100 loss: 0.027806073609712528\n",
      "Epoch 5200 loss: 0.026938152255244825\n",
      "Epoch 5300 loss: 0.026107924446377564\n",
      "Epoch 5400 loss: 0.02531368341761098\n",
      "Epoch 5500 loss: 0.024553772997101247\n",
      "Epoch 5600 loss: 0.02382659132665849\n",
      "Epoch 5700 loss: 0.0231305935590211\n",
      "Epoch 5800 loss: 0.0224642936576733\n",
      "Epoch 5900 loss: 0.021826265418000174\n",
      "Epoch 6000 loss: 0.02121514282145987\n",
      "Epoch 6100 loss: 0.020629619826439783\n",
      "Epoch 6200 loss: 0.0200684496906914\n",
      "Epoch 6300 loss: 0.01953044391099757\n",
      "Epoch 6400 loss: 0.019014470856360252\n",
      "Epoch 6500 loss: 0.018519454161797424\n",
      "Epoch 6600 loss: 0.018044370941044892\n",
      "Epoch 6700 loss: 0.017588249868242523\n",
      "Epoch 6800 loss: 0.01715016917114172\n",
      "Epoch 6900 loss: 0.016729254571567015\n",
      "Epoch 7000 loss: 0.016324677202796914\n",
      "Epoch 7100 loss: 0.01593565152819239\n",
      "Epoch 7200 loss: 0.015561433280752013\n",
      "Epoch 7300 loss: 0.015201317439258837\n",
      "Epoch 7400 loss: 0.014854636253252776\n",
      "Epoch 7500 loss: 0.014520757326153474\n",
      "Epoch 7600 loss: 0.014199081763406621\n",
      "Epoch 7700 loss: 0.013889042390486064\n",
      "Epoch 7800 loss: 0.013590102043892115\n",
      "Epoch 7900 loss: 0.013301751936900549\n",
      "Epoch 8000 loss: 0.013023510100688765\n",
      "Epoch 8100 loss: 0.012754919900565384\n",
      "Epoch 8200 loss: 0.012495548626310363\n",
      "Epoch 8300 loss: 0.012244986155075152\n",
      "Epoch 8400 loss: 0.01200284368486831\n",
      "Epoch 8500 loss: 0.011768752536332581\n",
      "Epoch 8600 loss: 0.011542363020296528\n",
      "Epoch 8700 loss: 0.011323343368430399\n",
      "Epoch 8800 loss: 0.011111378724246772\n",
      "Epoch 8900 loss: 0.010906170191643253\n",
      "Epoch 9000 loss: 0.010707433938182001\n",
      "Epoch 9100 loss: 0.010514900350328973\n",
      "Epoch 9200 loss: 0.010328313237927773\n",
      "Epoch 9300 loss: 0.010147429085252171\n",
      "Epoch 9400 loss: 0.009972016346066441\n",
      "Epoch 9500 loss: 0.009801854780216116\n",
      "Epoch 9600 loss: 0.009636734829370233\n",
      "Epoch 9700 loss: 0.009476457029644038\n",
      "Epoch 9800 loss: 0.009320831458933485\n",
      "Epoch 9900 loss: 0.009169677216903144\n"
     ]
    }
   ],
   "source": [
    "model = LinearNetwork()\n",
    "\n",
    "model.add_layer(DenseLayer(input_size=4,output_size=20))\n",
    "model.add_layer(DenseLayer(input_size=20,output_size=3))\n",
    "\n",
    "model.fit(X_train,y_train_onehot,epochs=10000,batch_size=32)\n",
    "\n",
    "y_pred_hot = model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred_hot,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_hot = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_hot,axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
